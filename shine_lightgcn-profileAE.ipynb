{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 改進 SHINE 模型，將Sentiment、Social Network Autoencoder 替換成 lightCGN, Profile 則維持 AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import timeit\n",
    "import numpy as np\n",
    "from prepare_data_new import Data\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Dense, Embedding, concatenate, Dot, Add\n",
    "from tensorflow.keras import backend as bk\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "# os.environ[\"path\"] += os.pathsep + 'c:/program files (x86)/graphviz2.38/bin'\n",
    "# saveDir = \"C:/Users/user/Google 雲端硬碟/NCKU_NETAI/SHINE/DGE/model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、讀取資料，包含:\n",
    "1. Sentiment Graph\n",
    "2. Social Relation Graph\n",
    "3. Profile Bipartite Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 1\n",
    "data = Data('data/sentiment.csv',\n",
    "            'data/social_relation.csv',\n",
    "            'data/celebrity_profile.csv',\n",
    "            'data/ordinary_user_profile.csv',\n",
    "            train_ratio,\n",
    "            random_state=0) # random_state 設 0 為SHINE原始碼所用之切割，效果會最好。\n",
    "holders_id_train, targets_id_train, holders_id_test, targets_id_test, holders_sen_adj_train, targets_sen_adj_train, y_train, holders_sen_adj_test, targets_sen_adj_test, y_test, adj_s = data.get_sentiment_data()\n",
    "holders_soc_adj_train, targets_soc_adj_train, holders_soc_adj_test, targets_soc_adj_test, adj_r = data.get_relation_data()\n",
    "holders_pro_train, targets_pro_train, holders_pro_test, targets_pro_test, adj_o, adj_c = data.get_profile_data()\n",
    "\n",
    "# 整數轉換成浮點數\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "holders_pro_train = holders_pro_train.astype(np.float32)\n",
    "targets_pro_train = targets_pro_train.astype(np.float32)\n",
    "holders_pro_test = holders_pro_test.astype(np.float32)\n",
    "targets_pro_test = targets_pro_test.astype(np.float32)\n",
    "\n",
    "num_user = adj_s.shape[0]\n",
    "num_data = len(holders_id_train)\n",
    "input_dim_adj_vec = holders_sen_adj_train.shape[1]\n",
    "input_dim_op = holders_pro_train.shape[1]\n",
    "input_dim_cp = targets_pro_train.shape[1]\n",
    "\n",
    "print(\"num of user:\", num_user)\n",
    "print(\"num of data:\", num_data)\n",
    "print(\"dim of ordinary\", input_dim_op)\n",
    "print(\"dim of celebrity\", input_dim_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、建構模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義超參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_size = 5\n",
    "batch_size = 1000\n",
    "l2_weight = 0.001\n",
    "lambda_pro = 1\n",
    "lambda_output = 10\n",
    "sen_act = 'relu'\n",
    "soc_act = 'sigmoid'\n",
    "pro_act = 'sigmoid'\n",
    "optimizer = 'adam'\n",
    "\n",
    "lr = 0.01\n",
    "EMBEDDING_DIM = 64\n",
    "n_layers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義 GCN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lightgcn_embed(adj, name, n_layers=3):\n",
    "    initializer = tf.initializers.GlorotUniform()\n",
    "    c_adj = bk.constant(adj)\n",
    "    ego_embeddings = tf.Variable(initializer([num_user, EMBEDDING_DIM]), name=name)\n",
    "    all_embeddings = [ego_embeddings]\n",
    "\n",
    "    for k in range(0, n_layers):\n",
    "        #將adj矩陣和ego相乘\n",
    "        side_embeddings = bk.dot(c_adj, ego_embeddings)\n",
    "        ego_embeddings = side_embeddings\n",
    "        all_embeddings += [ego_embeddings]\n",
    "\n",
    "    # 將每一層 GCN Layer 的結果全部 Concatenate 起來，可以保留不同視野下的資訊並減少 Over-Smooth問題\n",
    "    all_embeddings = tf.stack(all_embeddings, 1)\n",
    "    all_embeddings = tf.reduce_mean(all_embeddings, axis=1, keepdims=False)\n",
    "\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義幾個 Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put more weight on non-zero terms\n",
    "alpha = 10\n",
    "def reconstruction_loss(y_true, y_pred):\n",
    "    sqr = bk.square(y_pred - y_true)\n",
    "    weight = bk.abs(y_true) * (alpha - 1) + 1\n",
    "    return bk.sum(sqr * weight, axis=-1)\n",
    "\n",
    "def proximity_loss(y_true, y_pred):\n",
    "    return bk.sum(-y_pred * y_true, axis=-1)\n",
    "\n",
    "def proximity_loss_paper(y_true, y_pred):\n",
    "      return  bk.sum(bk.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義模型結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder definition\n",
    "holder_input = Input(name='holder_index_input', shape=(1), dtype=tf.int32)\n",
    "target_input = Input(name='target_index_input', shape=(1), dtype=tf.int32)\n",
    "\n",
    "########## Sentiment and Social GCN ##########\n",
    "# Use GCN layer to obtain the embedding table\n",
    "sen_emb_table = Embedding(num_user,\n",
    "                          EMBEDDING_DIM,\n",
    "                          embeddings_initializer=Constant(create_lightgcn_embed(adj_s, name=\"sen\", n_layers=n_layers)),\n",
    "                          embeddings_regularizer=l2(l2_weight),\n",
    "                          trainable=True,\n",
    "                          input_shape=(None, ))\n",
    "\n",
    "soc_emb_table = Embedding(num_user,\n",
    "                          EMBEDDING_DIM,\n",
    "                          embeddings_initializer=Constant(create_lightgcn_embed(adj_r, name=\"soc\", n_layers=n_layers)),\n",
    "                          embeddings_regularizer=l2(l2_weight),\n",
    "                          trainable=True,\n",
    "                          input_shape=(None, ))\n",
    "\n",
    "# Lookup the embedding table to obtain the embeddings of holder and target in both graph\n",
    "holder_sen_emb = sen_emb_table(holder_input)\n",
    "target_sen_emb = sen_emb_table(target_input)\n",
    "holder_soc_emb = soc_emb_table(holder_input)\n",
    "target_soc_emb = soc_emb_table(target_input)\n",
    "\n",
    "# dot product of two users\n",
    "sen_proximity = Dot(axes=-1, normalize=True)([holder_sen_emb, target_sen_emb])\n",
    "soc_proximity = Dot(axes=-1, normalize=True)([holder_soc_emb, target_soc_emb])\n",
    "\n",
    "\n",
    "########## Profile graph autoencoder ##########\n",
    "# o: ordinary people, c: celebrity\n",
    "# Input layer\n",
    "pro_o_input = Input(shape=(input_dim_op,))\n",
    "pro_c_input = Input(shape=(input_dim_cp,))\n",
    "\n",
    "# encoder\n",
    "pro_o_encoder = Dense(EMBEDDING_DIM, activation=pro_act, kernel_regularizer=l2(l2_weight))\n",
    "pro_c_encoder = Dense(EMBEDDING_DIM, activation=pro_act, kernel_regularizer=l2(l2_weight))\n",
    "pro_o_emb = pro_o_encoder(pro_o_input)\n",
    "pro_c_emb = pro_c_encoder(pro_c_input)\n",
    "\n",
    "# decoder\n",
    "pro_o_decoder = Dense(input_dim_op, activation=pro_act, kernel_regularizer=l2(l2_weight))\n",
    "pro_c_decoder = Dense(input_dim_cp, activation=pro_act, kernel_regularizer=l2(l2_weight))\n",
    "pro_recon_o = pro_o_decoder(pro_o_emb)\n",
    "pro_recon_c = pro_c_decoder(pro_c_emb)\n",
    "\n",
    "# dot product of two users\n",
    "pro_proximity = Dot(axes=-1, normalize=True)([pro_o_emb, pro_c_emb])\n",
    "\n",
    "\n",
    "\n",
    "########## Aggregation layer ##########\n",
    "# 最終預測情感值\n",
    "proximity = Add()([sen_proximity, soc_proximity, pro_proximity])\n",
    "\n",
    "\n",
    "\n",
    "# # Aggregate the embeddings by Concatenate\n",
    "# holder_agg_emb = concatenate([holder_sen_emb, holder_soc_emb])\n",
    "# target_agg_emb = concatenate([target_sen_emb, target_soc_emb])\n",
    "\n",
    "# # Predict the score by Dot Product\n",
    "# proximity = dot([holder_agg_emb, target_agg_emb], axes=-1, normalize=True)\n",
    "\n",
    "# 訓練用模型\n",
    "model = Model(inputs=[holder_input, target_input, pro_o_input, pro_c_input],\n",
    "              outputs=[proximity, pro_recon_o, pro_recon_c])\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=[proximity_loss, reconstruction_loss, reconstruction_loss],\n",
    "              loss_weights=[lambda_output, lambda_pro, lambda_pro])\n",
    "\n",
    "# 預測用模型\n",
    "predict_model = Model(inputs=[holder_input, target_input, pro_o_input, pro_c_input], outputs=proximity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "h = model.fit(x=[holders_id_train, targets_id_train, holders_pro_train, targets_pro_train],\n",
    "              y=[y_train, holders_pro_train, targets_pro_train],\n",
    "              epochs=epoch_size,\n",
    "              batch_size=batch_size,\n",
    "#               validation_split=0.1),\n",
    "              validation_data=([holders_id_test, targets_id_test, holders_pro_test, targets_pro_test],\n",
    "                               [y_test, holders_pro_test, targets_pro_test]))\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h.history['add_loss'], label='train_loss')\n",
    "plt.plot(h.history['val_add_loss'], label='val_loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 效果評估\n",
    "使用 Metric:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-Score\n",
    "- AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def result(X, Y, mode=''):\n",
    "    # 模型預測值\n",
    "    y_predict_raw = predict_model.predict(X).flatten()\n",
    "    y_predict = y_predict_raw.copy()\n",
    "    \n",
    "    # 計算中位數當作二分的門檻值\n",
    "    y_threshold = np.median(y_predict_raw)\n",
    "    y_predict[y_predict_raw < y_threshold] = -1\n",
    "    y_predict[y_predict_raw >= y_threshold] = 1\n",
    "    \n",
    "    # 用 Ground Truth 計算各種 Metrics\n",
    "    accuracy = accuracy_score(Y, y_predict)\n",
    "    f1 = f1_score(Y, y_predict, average='binary')\n",
    "    AUC = roc_auc_score(Y, y_predict_raw)\n",
    "    print(mode)\n",
    "    print(accuracy)\n",
    "    print(f1)\n",
    "    print(AUC)\n",
    "    print()\n",
    "\n",
    "print(\"Acc, F1, AUC:\")\n",
    "\n",
    "result(X=[holders_id_train, targets_id_train, holders_pro_train, targets_pro_train], Y=y_train, mode='Train')\n",
    "result(X=[holders_id_test, targets_id_test, holders_pro_test, targets_pro_test], Y=y_test, mode='Test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
