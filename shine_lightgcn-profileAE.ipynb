{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 改進 SHINE 模型，將Sentiment、Social Network Autoencoder 替換成 lightCGN, Profile 則維持 AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import timeit\n",
    "import numpy as np\n",
    "from prepare_data_new import Data\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Dense, Embedding, concatenate, Dot, Add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as bk\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "# os.environ[\"path\"] += os.pathsep + 'c:/program files (x86)/graphviz2.38/bin'\n",
    "# saveDir = \"C:/Users/user/Google 雲端硬碟/NCKU_NETAI/SHINE/DGE/model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、讀取資料，包含:\n",
    "1. Sentiment Graph\n",
    "2. Social Relation Graph\n",
    "3. Profile Bipartite Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of user: 12814\n",
      "num of data: 52336\n",
      "dim of ordinary 37\n",
      "dim of celebrity 108\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 1\n",
    "data = Data('data/sentiment.csv',\n",
    "            'data/social_relation.csv',\n",
    "            'data/celebrity_profile.csv',\n",
    "            'data/ordinary_user_profile.csv',\n",
    "            train_ratio,\n",
    "            random_state=20) # random_state 設 0 為SHINE原始碼所用之切割，效果會最好。\n",
    "holders_id_train, targets_id_train, holders_id_test, targets_id_test, holders_sen_adj_train, targets_sen_adj_train, y_train, holders_sen_adj_test, targets_sen_adj_test, y_test, adj_s = data.get_sentiment_data()\n",
    "holders_soc_adj_train, targets_soc_adj_train, holders_soc_adj_test, targets_soc_adj_test, adj_r = data.get_relation_data()\n",
    "holders_pro_train, targets_pro_train, holders_pro_test, targets_pro_test, adj_o, adj_c = data.get_profile_data()\n",
    "\n",
    "# 整數轉換成浮點數\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "holders_pro_train = holders_pro_train.astype(np.float32)\n",
    "targets_pro_train = targets_pro_train.astype(np.float32)\n",
    "holders_pro_test = holders_pro_test.astype(np.float32)\n",
    "targets_pro_test = targets_pro_test.astype(np.float32)\n",
    "\n",
    "num_user = adj_s.shape[0]\n",
    "num_data = len(holders_id_train)\n",
    "input_dim_adj_vec = holders_sen_adj_train.shape[1]\n",
    "input_dim_op = holders_pro_train.shape[1]\n",
    "input_dim_cp = targets_pro_train.shape[1]\n",
    "\n",
    "print(\"num of user:\", num_user)\n",
    "print(\"num of data:\", num_data)\n",
    "print(\"dim of ordinary\", input_dim_op)\n",
    "print(\"dim of celebrity\", input_dim_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、建構模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義超參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_size = 5\n",
    "batch_size = 1000\n",
    "l2_weight = 0\n",
    "lambda_pro = 1\n",
    "lambda_output = 10\n",
    "sen_act = 'relu'\n",
    "soc_act = 'sigmoid'\n",
    "pro_act = 'sigmoid'\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "EMBEDDING_DIM = 64\n",
    "n_layers = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義 GCN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lightgcn_embed(adj, name, n_layers=3):\n",
    "    initializer = tf.initializers.GlorotUniform()\n",
    "    c_adj = bk.constant(adj)\n",
    "    ego_embeddings = tf.Variable(initializer([num_user, EMBEDDING_DIM]), name=name)\n",
    "    all_embeddings = [ego_embeddings]\n",
    "\n",
    "    for k in range(0, n_layers):\n",
    "        #將adj矩陣和ego相乘\n",
    "        side_embeddings = bk.dot(c_adj, ego_embeddings)\n",
    "        ego_embeddings = side_embeddings\n",
    "        all_embeddings += [ego_embeddings]\n",
    "\n",
    "    # 將每一層 GCN Layer 的結果全部 Concatenate 起來，可以保留不同視野下的資訊並減少 Over-Smooth問題\n",
    "    all_embeddings = tf.stack(all_embeddings, 1)\n",
    "    all_embeddings = tf.reduce_mean(all_embeddings, axis=1, keepdims=False)\n",
    "\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義幾個 Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put more weight on non-zero terms\n",
    "alpha = 10\n",
    "def reconstruction_loss(y_true, y_pred):\n",
    "    sqr = bk.square(y_pred - y_true)\n",
    "    weight = bk.abs(y_true) * (alpha - 1) + 1\n",
    "    return bk.sum(sqr * weight, axis=-1)\n",
    "\n",
    "def proximity_loss(y_true, y_pred):\n",
    "    return bk.sum(-y_pred * y_true, axis=-1)\n",
    "\n",
    "def proximity_loss_paper(y_true, y_pred):\n",
    "      return  bk.sum(bk.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義模型結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder definition\n",
    "holder_input = Input(name='holder_index_input', shape=(1), dtype=tf.int32)\n",
    "target_input = Input(name='target_index_input', shape=(1), dtype=tf.int32)\n",
    "\n",
    "########## Sentiment and Social GCN ##########\n",
    "# Use GCN layer to obtain the embedding table\n",
    "sen_emb_table = Embedding(num_user,\n",
    "                          EMBEDDING_DIM,\n",
    "                          embeddings_initializer=Constant(create_lightgcn_embed(adj_s, name=\"sen\", n_layers=n_layers)),\n",
    "                          embeddings_regularizer=l2(l2_weight),\n",
    "                          trainable=True,\n",
    "                          input_shape=(None, ))\n",
    "\n",
    "soc_emb_table = Embedding(num_user,\n",
    "                          EMBEDDING_DIM,\n",
    "                          embeddings_initializer=Constant(create_lightgcn_embed(adj_r, name=\"soc\", n_layers=n_layers)),\n",
    "                          embeddings_regularizer=l2(l2_weight),\n",
    "                          trainable=True,\n",
    "                          input_shape=(None, ))\n",
    "\n",
    "# Lookup the embedding table to obtain the embeddings of holder and target in both graph\n",
    "holder_sen_emb = sen_emb_table(holder_input)\n",
    "target_sen_emb = sen_emb_table(target_input)\n",
    "holder_soc_emb = soc_emb_table(holder_input)\n",
    "target_soc_emb = soc_emb_table(target_input)\n",
    "\n",
    "# dot product of two users\n",
    "sen_proximity = Dot(axes=-1, normalize=True)([holder_sen_emb, target_sen_emb])\n",
    "soc_proximity = Dot(axes=-1, normalize=True)([holder_soc_emb, target_soc_emb])\n",
    "\n",
    "\n",
    "########## Profile graph autoencoder ##########\n",
    "# o: ordinary people, c: celebrity\n",
    "# Input layer\n",
    "pro_o_input = Input(shape=(input_dim_op,))\n",
    "pro_c_input = Input(shape=(input_dim_cp,))\n",
    "\n",
    "# encoder\n",
    "pro_o_encoder = Dense(EMBEDDING_DIM, activation=pro_act, kernel_regularizer=l2(l2_weight))\n",
    "pro_c_encoder = Dense(EMBEDDING_DIM, activation=pro_act, kernel_regularizer=l2(l2_weight))\n",
    "pro_o_emb = pro_o_encoder(pro_o_input)\n",
    "pro_c_emb = pro_c_encoder(pro_c_input)\n",
    "\n",
    "# decoder\n",
    "pro_o_decoder = Dense(input_dim_op, activation=pro_act, kernel_regularizer=l2(l2_weight))\n",
    "pro_c_decoder = Dense(input_dim_cp, activation=pro_act, kernel_regularizer=l2(l2_weight))\n",
    "pro_recon_o = pro_o_decoder(pro_o_emb)\n",
    "pro_recon_c = pro_c_decoder(pro_c_emb)\n",
    "\n",
    "# dot product of two users\n",
    "pro_proximity = Dot(axes=-1, normalize=True)([pro_o_emb, pro_c_emb])\n",
    "\n",
    "\n",
    "\n",
    "########## Aggregation layer ##########\n",
    "# 最終預測情感值\n",
    "proximity = Add()([sen_proximity, soc_proximity, pro_proximity])\n",
    "\n",
    "\n",
    "# 訓練用模型\n",
    "model = Model(inputs=[holder_input, target_input, pro_o_input, pro_c_input],\n",
    "              outputs=[proximity, pro_recon_o, pro_recon_c])\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=[proximity_loss, reconstruction_loss, reconstruction_loss],\n",
    "              loss_weights=[lambda_output, lambda_pro, lambda_pro])\n",
    "\n",
    "# 預測用模型\n",
    "predict_model = Model(inputs=[holder_input, target_input, pro_o_input, pro_c_input], outputs=proximity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/q56084098/.local/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 11ms/step - loss: 31.4211 - add_loss: -1.1093 - dense_2_loss: 9.4379 - dense_3_loss: 33.0757 - val_loss: 31.5415 - val_add_loss: -0.0714 - val_dense_2_loss: 6.9230 - val_dense_3_loss: 25.3322\n",
      "Epoch 2/5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 14.5257 - add_loss: -1.5489 - dense_2_loss: 6.3099 - dense_3_loss: 23.7047 - val_loss: 27.3309 - val_add_loss: -0.1777 - val_dense_2_loss: 6.0451 - val_dense_3_loss: 23.0631\n",
      "Epoch 3/5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 8.7582 - add_loss: -1.9120 - dense_2_loss: 5.7707 - dense_3_loss: 22.1077 - val_loss: 24.7062 - val_add_loss: -0.2574 - val_dense_2_loss: 5.6368 - val_dense_3_loss: 21.6434\n",
      "Epoch 4/5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 4.4596 - add_loss: -2.1537 - dense_2_loss: 5.3832 - dense_3_loss: 20.6139 - val_loss: 22.3411 - val_add_loss: -0.2918 - val_dense_2_loss: 5.2340 - val_dense_3_loss: 20.0255\n",
      "Epoch 5/5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 1.0303 - add_loss: -2.2854 - dense_2_loss: 4.9640 - dense_3_loss: 18.9201 - val_loss: 20.0102 - val_add_loss: -0.2994 - val_dense_2_loss: 4.7767 - val_dense_3_loss: 18.2280\n",
      "Time:  2.9099427219480276\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "h = model.fit(x=[holders_id_train, targets_id_train, holders_pro_train, targets_pro_train],\n",
    "              y=[y_train, holders_pro_train, targets_pro_train],\n",
    "              epochs=epoch_size,\n",
    "              batch_size=batch_size,\n",
    "#               validation_split=0.1),\n",
    "              validation_data=([holders_id_test, targets_id_test, holders_pro_test, targets_pro_test],\n",
    "                               [y_test, holders_pro_test, targets_pro_test]))\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'add_loss', 'dense_2_loss', 'dense_3_loss', 'val_loss', 'val_add_loss', 'val_dense_2_loss', 'val_dense_3_loss'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(h.history['add_loss'], label='train_loss')\n",
    "plt.plot(h.history['val_add_loss'], label='val_loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 效果評估\n",
    "使用 Metric:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-Score\n",
    "- AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc, F1, AUC:\n",
      "Train\n",
      "0.5908743503515744\n",
      "0.7084819605173588\n",
      "0.9408959817030929\n",
      "\n",
      "Test\n",
      "0.6763245033112583\n",
      "0.6763245033112583\n",
      "0.7392171670102188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def result(X, Y, mode=''):\n",
    "    # 模型預測值\n",
    "    y_predict_raw = predict_model.predict(X).flatten()\n",
    "    y_predict = y_predict_raw.copy()\n",
    "    \n",
    "    # 計算中位數當作二分的門檻值\n",
    "    y_threshold = np.median(y_predict_raw)\n",
    "    y_predict[y_predict_raw < y_threshold] = -1\n",
    "    y_predict[y_predict_raw >= y_threshold] = 1\n",
    "    \n",
    "    # 用 Ground Truth 計算各種 Metrics\n",
    "    accuracy = accuracy_score(Y, y_predict)\n",
    "    f1 = f1_score(Y, y_predict, average='binary')\n",
    "    AUC = roc_auc_score(Y, y_predict_raw)\n",
    "    print(mode)\n",
    "    print(accuracy)\n",
    "    print(f1)\n",
    "    print(AUC)\n",
    "    print()\n",
    "\n",
    "print(\"Acc, F1, AUC:\")\n",
    "\n",
    "result(X=[holders_id_train, targets_id_train, holders_pro_train, targets_pro_train], Y=y_train, mode='Train')\n",
    "result(X=[holders_id_test, targets_id_test, holders_pro_test, targets_pro_test], Y=y_test, mode='Test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "holder_index_input (InputLayer) [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "target_index_input (InputLayer) [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 37)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 108)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 64)        820096      holder_index_input[0][0]         \n",
      "                                                                 target_index_input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 64)        820096      holder_index_input[0][0]         \n",
      "                                                                 target_index_input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           2432        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           6976        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 1)         0           embedding[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 1)         0           embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1)            0           dense[0][0]                      \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 1, 1)         0           dot[0][0]                        \n",
      "                                                                 dot_1[0][0]                      \n",
      "                                                                 dot_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 37)           2405        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 108)          7020        dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,659,025\n",
      "Trainable params: 1,659,025\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
