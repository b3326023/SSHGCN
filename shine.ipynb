{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原始 SHINE 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dot, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as bk\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from prepare_data import Data\n",
    "import os\n",
    "import numpy as np\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "# os.environ[\"path\"] += os.pathsep + 'c:/program files (x86)/graphviz2.38/bin'\n",
    "# saveDir = \"C:/Users/user/Google 雲端硬碟/NCKU_NETAI/SHINE/DGE/model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、讀取資料，包含:\n",
    "1. Sentiment Graph\n",
    "2. Social Relation Graph\n",
    "3. Profile Bipartite Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 1\n",
    "data = Data('data/sentiment.csv',\n",
    "            'data/social_relation.csv',\n",
    "            'data/celebrity_profile.csv',\n",
    "            'data/ordinary_user_profile.csv',\n",
    "            train_ratio,\n",
    "           random_state=0) # random_state 設0為SHINE原始碼所用之切割，效果會最好，acc約有 80% 左右。\n",
    "\n",
    "X1_s_train, X2_s_train, y_train, X1_s_test, X2_s_test, y_test, _ = data.get_sentiment_data()\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "X1_r_train, X2_r_train, X1_r_test, X2_r_test, adj_matrix_r = data.get_relation_data()\n",
    "X1_p_train, X2_p_train, X1_p_test, X2_p_test, o_profile_one_hot, c_profile_one_hot = data.get_profile_data()\n",
    "\n",
    "input_dim_all_user = X1_s_train.shape[1]\n",
    "input_dim_op = X1_p_train.shape[1]\n",
    "input_dim_cp = X2_p_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 二、建構模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義幾個 Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put more weight on non-zero terms\n",
    "alpha = 10\n",
    "def reconstruction_loss(y_true, y_pred):\n",
    "    sqr = bk.square(y_pred - y_true)\n",
    "    weight = bk.abs(y_true) * (alpha - 1) + 1\n",
    "    return bk.sum(sqr * weight, axis=-1)\n",
    "\n",
    "def proximity_loss(y_true, y_pred):\n",
    "    return bk.sum(-y_pred * y_true, axis=-1)\n",
    "\n",
    "def proximity_loss_paper(y_true, y_pred):\n",
    "      return  bk.sum(bk.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義超參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_size = 5\n",
    "batch_size = 512\n",
    "EMBEDDING_DIM = 64\n",
    "l2_weight = 0.01\n",
    "lambda_sen = 1\n",
    "lambda_soc = 1\n",
    "lambda_pro = 1\n",
    "lambda_output = 30\n",
    "sen_act = 'tanh'\n",
    "soc_act = 'sigmoid'\n",
    "pro_act = 'sigmoid'\n",
    "optimizer = 'adam'\n",
    "recon_loss = reconstruction_loss\n",
    "proxi_loss = proximity_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義模型結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim_all_user = X1_s_train.shape[1]\n",
    "input_dim_op = X1_p_train.shape[1]\n",
    "input_dim_cp = X2_p_train.shape[1]\n",
    "\n",
    "########## Sentiment graph autoencoder ##########\n",
    "# Input layer\n",
    "sen_holder_input = Input(shape=(input_dim_all_user,))\n",
    "sen_target_input = Input(shape=(input_dim_all_user,))\n",
    "\n",
    "# encoder\n",
    "sen_encoder = Dense(EMBEDDING_DIM, activation=sen_act, kernel_regularizer=l2(l2_weight))\n",
    "sen_holder_emb = sen_encoder(sen_holder_input)\n",
    "sen_target_emb = sen_encoder(sen_target_input)\n",
    "\n",
    "# decoder\n",
    "sen_decoder = Dense(input_dim_all_user, activation=sen_act, kernel_regularizer=l2(l2_weight))\n",
    "sen_recon_holder = sen_decoder(sen_holder_emb)\n",
    "sen_recon_target = sen_decoder(sen_target_emb)\n",
    "\n",
    "# dot product of two users\n",
    "sen_proximity = Dot(axes=-1, normalize=True)([sen_holder_emb, sen_target_emb])\n",
    "\n",
    "\n",
    "########## Social relation graph autoencoder ##########\n",
    "# Input layer\n",
    "soc_holder_input = Input(shape=(input_dim_all_user,))\n",
    "soc_target_input = Input(shape=(input_dim_all_user,))\n",
    "\n",
    "# encoder\n",
    "soc_encoder = Dense(EMBEDDING_DIM, activation=soc_act, kernel_regularizer=l2(l2_weight))\n",
    "soc_holder_emb = soc_encoder(soc_holder_input)\n",
    "soc_target_emb = soc_encoder(soc_target_input)\n",
    "\n",
    "# decoder\n",
    "soc_decoder = Dense(input_dim_all_user, activation=soc_act, kernel_regularizer=l2(l2_weight))\n",
    "soc_recon_holder = soc_decoder(soc_holder_emb)\n",
    "soc_recon_target = soc_decoder(soc_target_emb)\n",
    "\n",
    "# dot product of two users\n",
    "soc_proximity = Dot(axes=-1, normalize=True)([soc_holder_emb, soc_target_emb])\n",
    "\n",
    "\n",
    "########## Profile graph autoencoder ##########\n",
    "# o: ordinary people, c: celebrity\n",
    "# Input layer\n",
    "pro_o_input = Input(shape=(input_dim_op,))\n",
    "pro_c_input = Input(shape=(input_dim_cp,))\n",
    "\n",
    "# encoder\n",
    "pro_o_encoder = Dense(EMBEDDING_DIM, activation=pro_act, kernel_regularizer=l2(l2_weight))\n",
    "pro_c_encoder = Dense(EMBEDDING_DIM, activation=pro_act, kernel_regularizer=l2(l2_weight))\n",
    "pro_o_emb = pro_o_encoder(pro_o_input)\n",
    "pro_c_emb = pro_c_encoder(pro_c_input)\n",
    "\n",
    "# decoder\n",
    "pro_o_decoder = Dense(input_dim_op, activation=pro_act, kernel_regularizer=l2(l2_weight))\n",
    "pro_c_decoder = Dense(input_dim_cp, activation=pro_act, kernel_regularizer=l2(l2_weight))\n",
    "pro_recon_o = pro_o_decoder(pro_o_emb)\n",
    "pro_recon_c = pro_c_decoder(pro_c_emb)\n",
    "\n",
    "# dot product of two users\n",
    "pro_proximity = Dot(axes=-1, normalize=True)([pro_o_emb, pro_c_emb])\n",
    "\n",
    "\n",
    "########## Aggregation layer ##########\n",
    "# 最終預測情感值\n",
    "proximity = Add()([sen_proximity, soc_proximity, pro_proximity])\n",
    "\n",
    "# 訓練用模型\n",
    "model = Model(inputs=[sen_holder_input, sen_target_input, soc_holder_input, soc_target_input, pro_o_input, pro_c_input],\n",
    "              outputs=[sen_recon_holder, sen_recon_target, soc_recon_holder, soc_recon_target, pro_recon_o, pro_recon_c, proximity])\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=[recon_loss, recon_loss, recon_loss,\n",
    "                    recon_loss, recon_loss, recon_loss, proxi_loss],\n",
    "              loss_weights=[lambda_sen, lambda_sen, lambda_soc, lambda_soc, lambda_pro, lambda_pro, lambda_output])\n",
    "\n",
    "# 預測用模型\n",
    "predict_model = Model(inputs=[sen_holder_input, sen_target_input, soc_holder_input, soc_target_input, pro_o_input, pro_c_input],\n",
    "                      outputs=proximity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "103/103 [==============================] - 21s 205ms/step - loss: 3425.7080 - dense_1_loss: 117.4875 - dense_1_1_loss: 423.8878 - dense_3_loss: 1282.7574 - dense_3_1_loss: 1582.3510 - dense_6_loss: 7.2077 - dense_7_loss: 27.6186 - add_loss: -2.0316 - val_loss: 1511.3995 - val_dense_1_loss: 87.2114 - val_dense_1_1_loss: 210.5212 - val_dense_3_loss: 341.6634 - val_dense_3_1_loss: 773.0676 - val_dense_6_loss: 6.1179 - val_dense_7_loss: 22.7360 - val_add_loss: -0.1516\n",
      "Epoch 2/3\n",
      "103/103 [==============================] - 24s 234ms/step - loss: 1350.3778 - dense_1_loss: 92.0806 - dense_1_1_loss: 190.1527 - dense_3_loss: 307.3583 - dense_3_1_loss: 708.7907 - dense_6_loss: 5.6958 - dense_7_loss: 21.4318 - add_loss: -2.0482 - val_loss: 1338.9807 - val_dense_1_loss: 79.6310 - val_dense_1_1_loss: 170.6544 - val_dense_3_loss: 269.5916 - val_dense_3_1_loss: 702.1226 - val_dense_6_loss: 5.7065 - val_dense_7_loss: 20.3604 - val_add_loss: -0.1418\n",
      "Epoch 3/3\n",
      "103/103 [==============================] - 13s 124ms/step - loss: 1259.5853 - dense_1_loss: 87.4014 - dense_1_1_loss: 169.5040 - dense_3_loss: 269.2322 - dense_3_1_loss: 670.5421 - dense_6_loss: 5.2867 - dense_7_loss: 18.9584 - add_loss: -2.0565 - val_loss: 1291.0105 - val_dense_1_loss: 77.1640 - val_dense_1_1_loss: 161.5461 - val_dense_3_loss: 258.5258 - val_dense_3_1_loss: 670.5148 - val_dense_6_loss: 5.2844 - val_dense_7_loss: 17.7654 - val_add_loss: -0.1469\n",
      "Time:  125.88634159002686\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "h = model.fit(x=[X1_s_train, X2_s_train, X1_r_train, X2_r_train, X1_p_train, X2_p_train],\n",
    "              y=[X1_s_train, X2_s_train, X1_r_train, X2_r_train, X1_p_train, X2_p_train, y_train],\n",
    "              epochs=epoch_size,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=([X1_s_test, X2_s_test, X1_r_test, X2_r_test, X1_p_test, X2_p_test],\n",
    "                               [X1_s_test, X2_s_test, X1_r_test, X2_r_test, X1_p_test, X2_p_test, y_test]),\n",
    "             verbose=1)\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 繪製訓練誤差與驗證誤差圖，此處只看最終預測值的誤差而不看 Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(h.history['add_loss'], label='train_loss')\n",
    "plt.plot(h.history['val_add_loss'], label='val_loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 效果評估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc, F1, AUC:\n",
      "Train\n",
      "0.6074594925099358\n",
      "0.7180036237852084\n",
      "0.9963254738625138\n",
      "\n",
      "Test\n",
      "0.8065573770491803\n",
      "0.8065573770491803\n",
      "0.8942287019618382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def result(X, Y, mode=''):\n",
    "    # 模型預測值\n",
    "    y_predict_raw = predict_model.predict(X).flatten()\n",
    "    y_predict = y_predict_raw.copy()\n",
    "    \n",
    "    # 計算中位數當作二分的門檻值\n",
    "    y_threshold = np.median(y_predict_raw)\n",
    "    y_predict[y_predict_raw < y_threshold] = -1\n",
    "    y_predict[y_predict_raw >= y_threshold] = 1\n",
    "    \n",
    "    # 用 Ground Truth 計算各種 Metrics\n",
    "    accuracy = accuracy_score(Y, y_predict)\n",
    "    f1 = f1_score(Y, y_predict, average='binary')\n",
    "    AUC = roc_auc_score(Y, y_predict_raw)\n",
    "    print(mode)\n",
    "    print(accuracy)\n",
    "    print(f1)\n",
    "    print(AUC)\n",
    "    print()\n",
    "\n",
    "\n",
    "print(\"Acc, F1, AUC:\")\n",
    "result(X=[X1_s_train, X2_s_train, X1_r_train, X2_r_train, X1_p_train, X2_p_train], Y=y_train, mode='Train')\n",
    "result(X=[X1_s_test, X2_s_test, X1_r_test, X2_r_test, X1_p_test, X2_p_test], Y=y_test, mode='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
